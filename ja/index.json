[{"authors":["admin"],"categories":null,"content":" 燈株式会社 DX ソリューション事業部 部長. 東京大学工学部計数工学科の学部在籍時から機械学習の研究と並行し, 東大松尾研究所にて複数のAIプロジェクトマネージャー・プロジェクト立案に従事. 同時に複数企業のデータ分析・エンジニアインターンも行う. 東京大学大学院情報理工学系研究科修士課程修了. 研究テーマは機械学習モデルを盗む攻撃に対する防御. Preferred Networks エンジニアとして機械学習アルゴリズム開発に従事したのちに, 現職. 研究や数々のプロジェクトを通じた幅広い技術的知見を軸として事業上の問題を抽出・解決することを生業とする.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"ja","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://d-dof.github.io/ja/author/%E6%A3%AE-%E9%9B%84%E4%BA%BA-/-yuto-mori/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ja/author/%E6%A3%AE-%E9%9B%84%E4%BA%BA-/-yuto-mori/","section":"authors","summary":"燈株式会社 DX ソリューション事業部 部長. 東京大学工学部計数工学科の学部在籍時から機械学習の研究と並行し, 東大松尾研究所にて複数のAIプロジェクトマネージャー・プロジェクト立案に従事. 同時に複数企業のデータ分析・エンジニアインターンも行う. 東京大学大学院情報理工学系研究科修士課程修了. 研究テーマは機械学習モデルを盗む攻撃に対する防御. Preferred Networks エンジニアとして機械学習アルゴリズム開発に従事したのちに, 現職. 研究や数々のプロジェクトを通じた幅広い技術的知見を軸として事業上の問題を抽出・解決することを生業とする.","tags":null,"title":"森 雄人 / Yuto Mori","type":"authors"},{"authors":[],"categories":[],"content":"arXiv にて論文を公開しました.\nYuto Mori, Atsushi Nitanda, and Akiko Takeda. BODAME: Bilevel Optimization for Defense Against Model Extraction. 2021. [arXiv]\n題名の通り, モデル抽出攻撃 (Model Extraction) と呼ばれる攻撃から機械学習モデルを防御する問題を二段階最適化として定式化し, その最適化方法について提案したものです.\nまたそれに伴って, 修士の頃にまとめていたサーベイ資料の一部を公開しました.\n  PDF  GitHub レポジトリ  コロナ禍に本格的に入っていくという段階でコツコツ日記代わりにまとめていたサーベイです. 主としてモデル抽出攻撃に関連するトピックについてまとめていますが, 次のようなトピックについての論文要約も一部入っています.\n Active Learning Semi-supervised Learning Kernel Methods Machine Teaching Gaussian Process Poisoning Meta-Learning  ","date":1621681082,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1621681082,"objectID":"1edfbca3413eb6b7a026fcf3c96a7824","permalink":"https://d-dof.github.io/ja/post/bodame/","publishdate":"2021-05-22T19:58:02+09:00","relpermalink":"/ja/post/bodame/","section":"post","summary":"arXiv にて論文を公開しました \u0026 サーベイを GitHub にて公開しました","tags":["Publication","Model Extraction","Survey"],"title":"BODAME: Bilevel Optimization for Defense Against Model Extraction","type":"post"},{"authors":[],"categories":[],"content":"「二段階最適化によるモデル抽出攻撃に対する防御」という題で 第23回情報論的学習理論ワークショップ (2020-11-23〜26, IBIS2020) にて発表しました.\n今年はコロナ禍の中ということもありオンライン開催でしたが, Slack + Vimeoによる録画の事前投稿というスタイルで, チュートリアル・企画セッションを含め非常に密なやり取りを行うことができました. 大変面白かったです.\n追記: 優秀発表賞ファイナリスト (12件/全発表116件) に選ばれました. https://ibisml.org/ibis2020/awards/\n","date":1607838648,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1607838648,"objectID":"4b48a2bb7932f2685da7e19c741d0f32","permalink":"https://d-dof.github.io/ja/post/ibis-2020/","publishdate":"2020-12-13T14:50:48+09:00","relpermalink":"/ja/post/ibis-2020/","section":"post","summary":"第23回情報論的学習理論ワークショップ (IBIS2020) での発表について","tags":[],"title":"IBISで発表しました","type":"post"},{"authors":[],"categories":[],"content":"自分の備忘録も兼ねて, カジュアルに機械学習のちょっとした Tips をまとめるシリーズを始めてみたいと思います. 第一弾はカーネルモデルに対する近似手法について.\nカーネルモデルを近似する方法としていくつかの方法 (Nyström近似など) がありますが, そのうちランダムな基底 (特徴) を用いて近似する手法を Random Feature といいます. これは [Rahimi and Recht, NeurIPS, 2008]によって提案された手法で, アイディアはシンプルですが非常に面白いです.\nBochner の定理と Random Feature  $\\mathbb{R}^d$ 上の連続な平行移動不変カーネル $k(x, y) = k(x - y)$ が正定値であることの必要十分条件は $k(\\delta)$ が非負測度のフーリエ変換となることである.\n これは Bochner の定理と呼ばれるものです. この結果から具体的な表式として, 平行移動不変なカーネル関数はある確率分布 $p(\\omega)$ を用いて次のように表現することができます. \\begin{align} k(x-y) \u0026amp;= \\int_{\\mathbb{R}^d} p(\\omega) e^{i \\omega^{\\top}(x-y)} d \\omega\\\\\n\u0026amp;= \\mathbb{E} [e^{i \\omega^{\\top}x} \\overline{e^{i \\omega^{\\top} y}}] \\end{align}\nここでさらに, 実用上実数値カーネルを考えれば十分であることに注意し, 加法定理を用いて整理すると \\begin{align} k(x-y) \u0026amp;= \\mathbb{E}[\\sqrt{2} \\cos (\\omega^{\\top}x + b) \\sqrt{2} \\cos (\\omega^{\\top}y + b)]\\\\\n\u0026amp;\\approx \\frac{1}{D} \\sum_{j=1}^{D} \\sqrt{2} \\cos (\\omega^{\\top}_{j} x + b_j) \\sqrt{2} \\cos (\\omega^{\\top}_{j} y + b_j) \\end{align} と表すことができます. なおここで $\\omega, \\omega_j \\sim p(\\omega), b, b_j \\sim U[0, 2\\pi]$ です. このとき, 上の期待値の不偏推定量による近似をシンプルに考えています.\nさて, ここで\n\\begin{align} z(x) = \\begin{pmatrix} \\sqrt{\\frac{2}{D}} \\cos(\\omega_1^{\\top} x + b_1)\\\\\n\\vdots \\\\\n\\sqrt{\\frac{2}{D}} \\cos(\\omega_D^{\\top} x + b_D) \\end{pmatrix} \\end{align} という関数を考えると, ランダムに $x$ を $\\mathbb{R}^D$ 上へ埋め込んでいる関数になっており, 先ほどの推定量は $z(x)^{\\top} z(y)$ と書くことができます. この $z(x)$ こそが Random Features であって, 「(一般には無限次元空間への写像となりがちな) 特徴写像をランダムに (低次元空間への写像として) 近似したもの」と見ることもできるというカラクリです. 非常に面白いですね.\nReferences   Ali Rahimi and Ben Recht. Random features for large-scale kernel machines. Advances in Neural Information Processing Systems, 2008.  Wikipedia: Bochner\u0026rsquo;s theorem http://gregorygundersen.com/blog/2019/12/23/random-fourier-features/  ","date":1601265160,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1601265160,"objectID":"7e79f61a467e89be871621dfb6b749f9","permalink":"https://d-dof.github.io/ja/post/random-feature/","publishdate":"2020-09-28T12:52:40+09:00","relpermalink":"/ja/post/random-feature/","section":"post","summary":"Random Feature についてのまとめ","tags":["Kernel","ML-Tips","Random Feature","Famous Topic"],"title":"カーネルモデルを近似する手法: Random Feature","type":"post"},{"authors":[],"categories":[],"content":"このポートフォリオサイトを開設しました.\n","date":1587791238,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1587791238,"objectID":"b3217fe2c9870a6f194a145697561e25","permalink":"https://d-dof.github.io/ja/post/1-intro/","publishdate":"2020-04-25T14:07:18+09:00","relpermalink":"/ja/post/1-intro/","section":"post","summary":"このポートフォリオサイトを開設しました.","tags":[],"title":"開設 !!!","type":"post"}]